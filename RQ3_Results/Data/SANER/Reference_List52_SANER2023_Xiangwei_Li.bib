@misc{ref1,
  author = {Tomáš Mikolov and Kai Chen and Greg S. Corrado and Jay B. Dean},
  title = {Efficient Estimation of Word Representations in Vector Space},
  year = {2013}
}

@misc{ref2,
  author = {Robert M. French},
  title = {Catastrophic forgetting in connectionist networks},
  year = {1999}
}

@misc{ref3,
  author = {Wilson L. Taylor},
  title = {“Cloze Procedure”: A New Tool for Measuring Readability},
  year = {1953}
}

@misc{ref4,
  author = {Peng Zhou and Wei Shi and Jun Tian and Zhenyu Qi and Bingchen Li and Hongwei Hao and Bo Xu},
  title = {Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification},
  year = {2016}
}

@misc{ref5,
  author = {Zhuobing Han and Xiaohong Li and Zhenchang Xing and Hongtao Liu and Zhiyong Feng},
  title = {Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description},
  year = {2017}
}

@misc{ref6,
  author = {Jacob Devlin and Ming‐Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2018}
}

@misc{ref7,
  author = {Jeremy Howard and Sebastian Ruder},
  title = {Universal Language Model Fine-tuning for Text Classification},
  year = {2018}
}

@misc{ref8,
  author = {Qianru Sun and Yaoyao Liu and Tat‐Seng Chua and Bernt Schiele},
  title = {Meta-Transfer Learning for Few-Shot Learning},
  year = {2019}
}

@misc{ref9,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = {2019}
}

@misc{ref10,
  author = {Fabio Petroni and Tim Rocktäschel and Sebastian Riedel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander Miller},
  title = {Language Models as Knowledge Bases?},
  year = {2019}
}

@misc{ref11,
  author = {Yaqin Zhou and Shangqing Liu and Jingkai Siow and Xiaoning Du and Yang Liu},
  title = {Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks},
  year = {2019}
}

@misc{ref12,
  author = {Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
  title = {How to Fine-Tune BERT for Text Classification?},
  year = {2019}
}

@misc{ref13,
  author = {Xi Gong and Zhenchang Xing and Xiaohong Li and Zhiyong Feng and Zhuobing Han},
  title = {Joint Prediction of Multiple Vulnerability Characteristics Through Multi-Task Learning},
  year = {2019}
}

@misc{ref14,
  author = {Yaqing Wang and Quanming Yao and James T. Kwok and Lionel M. Ni},
  title = {Generalizing from a Few Examples},
  year = {2020}
}

@misc{ref15,
  author = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
  title = {A Comprehensive Survey on Transfer Learning},
  year = {2020}
}

@misc{ref16,
  author = {Zhengbao Jiang and Frank F. Xu and Jun Araki and Graham Neubig},
  title = {How Can We Know What Language Models Know?},
  year = {2020}
}

@misc{ref17,
  author = {Cody Watson and Nathan Cooper and David Nader Palacio and Kevin Moran and Denys Poshyvanyk},
  title = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research},
  year = {2022}
}

@misc{ref18,
  author = {Xipeng Qiu and Tianxiang Sun and Yige Xu and Yunfan Shao and Ning Dai and Xuanjing Huang},
  title = {Pre-trained models for natural language processing: A survey},
  year = {2020}
}

@misc{ref19,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref20,
  author = {Tianyi Zhang and Felix Wu and Arzoo Katiyar and Kilian Q. Weinberger and Yoav Artzi},
  title = {Revisiting Few-sample BERT Fine-tuning},
  year = {2021}
}

@misc{ref21,
  author = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and A. Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Sheng‐Yu Fu and Shujie Liu},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  year = {2021}
}

@misc{ref22,
  author = {Timo Schick and Hinrich Schütze},
  title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  year = {2021}
}

@misc{ref23,
  author = {Timothy M. Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
  title = {Meta-Learning in Neural Networks: A Survey},
  year = {2021}
}

@misc{ref24,
  author = {Ethan Perez and Douwe Kiela and Kyunghyun Cho},
  title = {True Few-Shot Learning with Language Models},
  year = {2021}
}

@misc{ref25,
  author = {Guanghui Qin and Jason Eisner},
  title = {Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},
  year = {2021}
}

@misc{ref26,
  author = {Zexuan Zhong and Dan Friedman and Danqi Chen},
  title = {Factual Probing Is [MASK]: Learning vs. Learning to Recall},
  year = {2021}
}

@misc{ref27,
  author = {Xiang Lisa Li and Percy Liang},
  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  year = {2021}
}

@misc{ref28,
  author = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year = {2022}
}

@misc{ref29,
  author = {Yuxian Gu and Xu Han and Zhiyuan Liu and Minlie Huang},
  title = {PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  year = {2022}
}

@misc{ref30,
  author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  title = {Pre-trained models: Past, present and future},
  year = {2021}
}

@misc{ref31,
  author = {Ning Ding and Shengding Hu and Weilin Zhao and Yulin Chen and Zhiyuan Liu and Hai-Tao Zheng and Maosong Sun},
  title = {OpenPrompt: An Open-source Framework for Prompt-learning},
  year = {2022}
}

@misc{ref32,
  author = {Siddhartha Shankar Das and Edoardo Serra and Mahantesh Halappanavar and Alex Pothen and Ehab Al‐Shaer},
  title = {V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities},
  year = {2021}
}

@misc{ref33,
  author = {Brian Lester and Rami Al‐Rfou and Noah Constant},
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  year = {2021}
}

@misc{ref34,
  author = {Yanming Yang and Xin Xia and David Lo and John Grundy},
  title = {A Survey on Deep Learning for Software Engineering},
  year = {2021}
}

@misc{ref35,
  author = {Jesse Dodge and Gabriel Ilharco and Roy Schwartz and Ali Farhadi and Hannaneh Hajishirzi and Noah A. Smith},
  title = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping},
  year = {2020}
}

@misc{ref36,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{ref37,
  author = {T. B. Brown and Benjamin F. Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey C.S. Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric J. Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack A. Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {Language Models are Few-Shot Learners},
  year = {2020}
}

@misc{ref38,
  author = {Xu Han and Weilin Zhao and Ning Ding and Zhiyuan Liu and Maosong Sun},
  title = {PTR: Prompt Tuning with Rules for Text Classification},
  year = {2022}
}

@misc{ref39,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

