@misc{ref1,
  author = {Thang Luong and Hieu Pham and Christopher D. Manning},
  title = {Effective Approaches to Attention-based Neural Machine Translation},
  year = {2015}
}

@misc{ref2,
  author = {Sepp Hochreiter and Jürgen Schmidhuber},
  title = {Long Short-Term Memory},
  year = {1997}
}

@misc{ref3,
  author = {Kishore Papineni and Salim Roukos and Todd J. Ward and Wei-Jing Zhu},
  title = {BLEU},
  year = {2001}
}

@misc{ref4,
  author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  title = {Sequence to Sequence Learning with Neural Networks},
  year = {2014}
}

@misc{ref5,
  author = {Alon Lavie and Abhaya Agarwal},
  title = {Meteor},
  year = {2007}
}

@misc{ref6,
  author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year = {2014}
}

@misc{ref7,
  author = {Raia Hadsell and Sumit Chopra and Yann LeCun},
  title = {Dimensionality Reduction by Learning an Invariant Mapping},
  year = {2006}
}

@misc{ref8,
  author = {Chin-Yew Lin},
  title = {ROUGE: A Package for Automatic Evaluation of Summaries},
  year = {2004}
}

@misc{ref9,
  author = {Kyunghyun Cho and Bart van Merriënboer and Çağlar Gülçehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  title = {Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
  year = {2014}
}

@misc{ref10,
  author = {Sinno Jialin Pan and Qiang Yang},
  title = {A Survey on Transfer Learning},
  year = {2009}
}

@misc{ref11,
  author = {Srinivasan Iyer and Ioannis Konstas and Alvin Cheung and Luke Zettlemoyer},
  title = {Summarizing Source Code using a Neural Attention Model},
  year = {2016}
}

@misc{ref12,
  author = {Yongqin Xian and Bernt Schiele and Zeynep Akata},
  title = {Zero-Shot Learning — The Good, the Bad and the Ugly},
  year = {2017}
}

@misc{ref13,
  author = {Abigail See and Peter J. Liu and Christopher D. Manning},
  title = {Get To The Point: Summarization with Pointer-Generator Networks},
  year = {2017}
}

@misc{ref14,
  author = {Shikhar Sharma and Layla El Asri and Hannes Schulz and Jérémie Zumer},
  title = {Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},
  year = {2017}
}

@misc{ref15,
  author = {Qingyu Zhou and Nan Yang and Furu Wei and Ming Zhou},
  title = {Sequential Copying Networks},
  year = {2018}
}

@misc{ref16,
  author = {Uri Alon and Meital Zilberstein and Omer Levy and Eran Yahav},
  title = {A general path-based representation for predicting program properties},
  year = {2018}
}

@misc{ref17,
  author = {Xing Hu and Ge Li and Xin Xia and David Lo and Zhi Jin},
  title = {Deep code comment generation},
  year = {2018}
}

@misc{ref18,
  author = {Yao Wan and Zhou Zhao and Min Yang and Guandong Xu and Haochao Ying and Jian Wu and Philip S. Yu},
  title = {Improving automatic source code summarization via deep reinforcement learning},
  year = {2018}
}

@misc{ref19,
  author = {Sebastian Gehrmann and Yuntian Deng and Alexander M. Rush},
  title = {Bottom-Up Abstractive Summarization},
  year = {2018}
}

@misc{ref20,
  author = {Jacob Devlin and Ming‐Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2018}
}

@misc{ref21,
  author = {Ke Wang},
  title = {Learning Scalable and Precise Representation of Program Semantics},
  year = {2019}
}

@misc{ref22,
  author = {Xing Hu and Ge Li and Xin Xia and David Lo and Zhi Jin},
  title = {Deep code comment generation with hybrid lexical and syntactical information},
  year = {2019}
}

@misc{ref23,
  author = {Yujia Li and Daniel Tarlow and Marc Brockschmidt and Richard S. Zemel},
  title = {Gated Graph Sequence Neural Networks},
  year = {2015}
}

@misc{ref24,
  author = {Kaitao Song and Xu Tan and Tao Qin and Jianfeng Lu and Tie‐Yan Liu},
  title = {MASS: Masked Sequence to Sequence Pre-training for Language Generation},
  year = {2019}
}

@misc{ref25,
  author = {Jian Zhang and Xu Wang and Hongyu Zhang and Hailong Sun and Kaixuan Wang and Xudong Liu},
  title = {A Novel Neural Source Code Representation Based on Abstract Syntax Tree},
  year = {2019}
}

@misc{ref26,
  author = {Ke Wang and Zhendong Su},
  title = {Learning Blended, Precise Semantic Program Embeddings},
  year = {2019}
}

@misc{ref27,
  author = {Alexander M. Rush and Sumit Chopra and Jason Weston},
  title = {A Neural Attention Model for Abstractive Sentence Summarization},
  year = {2015}
}

@misc{ref28,
  author = {Zhaopeng Tu and Zhengdong Lu and Yang Liu and Xiaohua Liu and Hang Li},
  title = {Modeling Coverage for Neural Machine Translation},
  year = {2016}
}

@misc{ref29,
  author = {Miltiadis Allamanis and Marc Brockschmidt and Mahmoud Khademi},
  title = {Learning to Represent Programs with Graphs},
  year = {2017}
}

@misc{ref30,
  author = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  title = {Self-Attention with Relative Position Representations},
  year = {2018}
}

@misc{ref31,
  author = {Alexander LeClair and Siyuan Jiang and Collin McMillan},
  title = {A Neural Model for Generating Natural Language Summaries of Program Subroutines},
  year = {2019}
}

@misc{ref32,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = {2019}
}

@misc{ref33,
  author = {Yuxiang Zhu and Minxue Pan},
  title = {Automatic Code Summarization: A Systematic Literature Review},
  year = {2019}
}

@misc{ref34,
  author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clément Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  title = {Transformers: State-of-the-Art Natural Language Processing},
  year = {2020}
}

@misc{ref35,
  author = {Sewon Min and Danqi Chen and Luke Zettlemoyer and Hannaneh Hajishirzi},
  title = {Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering},
  year = {2019}
}

@misc{ref36,
  author = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
  title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  year = {2019}
}

@misc{ref37,
  author = {Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
  title = {Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  year = {2020}
}

@misc{ref38,
  author = {Yada Pruksachatkun and Jason Phang and Haokun Liu and Phu Mon Htut and Xiaoyi Zhang and Richard Yuanzhe Pang and Clara Vania and Katharina Kann and Samuel R. Bowman},
  title = {Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?},
  year = {2020}
}

@misc{ref39,
  author = {Song Xu and Haoran Li and Peng Yuan and Youzheng Wu and Xiaodong He and Bowen Zhou},
  title = {Self-Attention Guided Copy Mechanism for Abstractive Summarization},
  year = {2020}
}

@misc{ref40,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {A Transformer-based Approach for Source Code Summarization},
  year = {2020}
}

@misc{ref41,
  author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  year = {2020}
}

@misc{ref42,
  author = {Bei Li and Hui Liu and Ziyang Wang and Yufan Jiang and Tong Xiao and Jingbo Zhu and Tongran Liu and Changliang Li},
  title = {Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation},
  year = {2020}
}

@misc{ref43,
  author = {Sascha Rothe and Shashi Narayan and Aliaksei Severyn},
  title = {Leveraging Pre-trained Checkpoints for Sequence Generation Tasks},
  year = {2020}
}

@misc{ref44,
  author = {Jian Zhang and Xu Wang and Hongyu Zhang and Hailong Sun and Xudong Liu},
  title = {Retrieval-based neural source code summarization},
  year = {2020}
}

@misc{ref45,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref46,
  author = {Thommen George Karimpanal and Roland Bouffanais},
  title = {Self-organizing maps for storage and transfer of knowledge in reinforcement learning},
  year = {2018}
}

@misc{ref47,
  author = {Stéphane Aroca-Ouellette and Frank Rudzicz},
  title = {On Losses for Modern Language Models},
  year = {2020}
}

@misc{ref48,
  author = {Jonathan Mallinson and Aliaksei Severyn and Eric Malmi and Guillermo Carrascón},
  title = {FELIX: Flexible Text Editing Through Tagging and Insertion},
  year = {2020}
}

@misc{ref49,
  author = {David Gros and Hariharan Sezhiyan and Prémkumar Dévanbu and Yu Zhou},
  title = {Code to comment "translation"},
  year = {2020}
}

@misc{ref50,
  author = {Shangqing Liu and Yu Chen and Xiaofei Xie and Jing Kai Siow and Yang Liu},
  title = {Retrieval-Augmented Generation for Code Summarization via Hybrid GNN},
  year = {2020}
}

@misc{ref51,
  author = {Armen Aghajanyan and Anchit Gupta and Akshat Shrivastava and Xilun Chen and Luke Zettlemoyer and Sonal Gupta},
  title = {Muppet: Massive Multi-task Representations with Pre-Finetuning},
  year = {2021}
}

@misc{ref52,
  author = {Triet Huynh Minh Le and Hao Chen and Muhammad Ali Babar},
  title = {Deep Learning for Source Code Modeling and Generation},
  year = {2020}
}

@misc{ref53,
  author = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and A. Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Sheng‐Yu Fu and Shujie Liu},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  year = {2021}
}

@misc{ref54,
  author = {Shuteng Niu and Yongxin Liu and Jian Wang and Houbing Song},
  title = {A Decade Survey of Transfer Learning (2010–2020)},
  year = {2020}
}

@misc{ref55,
  author = {Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},
  title = {SummEval: Re-evaluating Summarization Evaluation},
  year = {2021}
}

@misc{ref56,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {Unified Pre-training for Program Understanding and Generation},
  year = {2021}
}

@misc{ref57,
  author = {Chen Lin and Zhichao Ouyang and Junqing Zhuang and Jianqiang Chen and Hui Li and Rongxin Wu},
  title = {Improving Code Summarization with Block-wise Abstract Syntax Tree Splitting},
  year = {2021}
}

@misc{ref58,
  author = {Hongqiu Wu and Hai Zhao and Min Zhang},
  title = {Code Summarization with Structure-induced Transformer},
  year = {2021}
}

@misc{ref59,
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pondé de Oliveira Pinto and Jared Kaplan and Harrison Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Łukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth A. Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and I. Babuschkin and Suchir Balaji and Shantanu Jain and William S. Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title = {Evaluating Large Language Models Trained on Code},
  year = {2021}
}

@misc{ref60,
  author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ B. Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dorottya Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tong Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Ahmad Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin T. Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon-Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Yang Wang and Bo-Han Wu and Jia-Jun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  title = {On the Opportunities and Risks of Foundation Models},
  year = {2021}
}

@misc{ref61,
  author = {Devjeet Roy and Sarah Fakhoury and Venera Arnaoudova},
  title = {Reassessing automatic evaluation metrics for code summarization tasks},
  year = {2021}
}

@misc{ref62,
  author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  title = {Pre-trained models: Past, present and future},
  year = {2021}
}

@misc{ref63,
  author = {Jian Gu and Zimin Chen and Martin Monperrus},
  title = {Multimodal Representation for Neural Code Search},
  year = {2021}
}

@misc{ref64,
  author = {Paras Jain and Ajay N. Jain and Tianjun Zhang and Pieter Abbeel and Joseph E. Gonzalez and Ion Stoica},
  title = {Contrastive Code Representation Learning},
  year = {2021}
}

@misc{ref65,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{ref66,
  author = {Shane Storks and Qiaozi Gao and Joyce Chai},
  title = {Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches},
  year = {2019}
}

@misc{ref67,
  author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James T. Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year = {2019}
}

@misc{ref68,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

@misc{ref69,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

