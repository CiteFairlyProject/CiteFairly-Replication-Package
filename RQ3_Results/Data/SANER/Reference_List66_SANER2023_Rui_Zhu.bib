@misc{ref1,
  author = {Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  year = {2019}
}

@misc{ref2,
  author = {Srinivasan Iyer and Ioannis Konstas and Alvin Cheung and Luke Zettlemoyer},
  title = {Mapping Language to Code in Programmatic Context},
  year = {2018}
}

@misc{ref3,
  author = {Nicholas Carlini and David Wagner},
  title = {Towards Evaluating the Robustness of Neural Networks},
  year = {2017}
}

@misc{ref4,
  author = {Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},
  title = {Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment},
  year = {2020}
}

@misc{ref5,
  author = {Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
  title = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  year = {2020}
}

@misc{ref6,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref7,
  author = {Linyang Li and Ruotian Ma and Qipeng Guo and Xiangyang Xue and Xipeng Qiu},
  title = {BERT-ATTACK: Adversarial Attack Against BERT Using BERT},
  year = {2020}
}

@misc{ref8,
  author = {Colin Clement and Dawn Drain and Jonathan Timcheck and A. Svyatkovskiy and Neel Sundaresan},
  title = {PyMT5: multi-mode translation of natural language and Python code with transformers},
  year = {2020}
}

@misc{ref9,
  author = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Wilmer Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  year = {2021}
}

@misc{ref10,
  author = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and A. Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Sheng‐Yu Fu and Shujie Liu},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  year = {2021}
}

@misc{ref11,
  author = {Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  title = {Measuring Coding Challenge Competence With APPS},
  year = {2021}
}

@misc{ref12,
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pondé de Oliveira Pinto and Jared Kaplan and Harrison Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Łukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth A. Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and I. Babuschkin and Suchir Balaji and Shantanu Jain and William S. Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title = {Evaluating Large Language Models Trained on Code},
  year = {2021}
}

@misc{ref13,
  author = {Yiyang Hao and Ge Li and Yongqiang Liu and Xiaowei Miao and He Zong and Siyuan Jiang and Yang Liu and Wei He},
  title = {AixBench: A Code Generation Benchmark Dataset},
  year = {2022}
}

@misc{ref14,
  author = {T. B. Brown and Benjamin F. Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey C.S. Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric J. Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack A. Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {Language Models are Few-Shot Learners},
  year = {2020}
}

@misc{ref15,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

