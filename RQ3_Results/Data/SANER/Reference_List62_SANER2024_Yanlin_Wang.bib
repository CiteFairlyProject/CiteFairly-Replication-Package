@misc{ref1,
  author = {Paul W. McBurney and Collin McMillan},
  title = {Automatic documentation generation via source code summarization of method context},
  year = {2014}
}

@misc{ref2,
  author = {Brian P. Eddy and Jeffrey A. Robinson and Nicholas A. Kraft and Jeffrey C. Carver},
  title = {Evaluating source code summarization techniques: Replication and expansion},
  year = {2013}
}

@misc{ref3,
  author = {Jeffrey Svajlenko and Judith F. Islam and Iman Keivanloo and Chanchal K. Roy and Mohammad Mamun Mia},
  title = {Towards a Big Data Curated Benchmark of Inter-project Code Clones},
  year = {2014}
}

@misc{ref4,
  author = {Giriprasad Sridhara and Emily Hill and Divya Muppaneni and Lori Pollock and K. Vijay‐Shanker},
  title = {Towards automatically generating summary comments for Java methods},
  year = {2010}
}

@misc{ref5,
  author = {Paige Rodeghero and Collin McMillan and Paul W. McBurney and Nigel Bosch and Sidney D’Mello},
  title = {Improving automated source code summarization via an eye-tracking study of programmers},
  year = {2014}
}

@misc{ref6,
  author = {Dana Movshovitz‐Attias and William W. Cohen},
  title = {Natural Language Models for Predicting Programming Comments},
  year = {2013}
}

@misc{ref7,
  author = {Sonia Haiduc and Jairo Aponte and Laura Moreno and Andrian Marcus},
  title = {On the Use of Automated Text Summarization Techniques for Summarizing Source Code},
  year = {2010}
}

@misc{ref8,
  author = {Sonia Haiduc and Jairo Aponte and Andrian Marcus},
  title = {Supporting program comprehension with source code summarization},
  year = {2010}
}

@misc{ref9,
  author = {Chin-Yew Lin and Franz Josef Och},
  title = {ORANGE},
  year = {2004}
}

@misc{ref10,
  author = {Srinivasan Iyer and Ioannis Konstas and Alvin Cheung and Luke Zettlemoyer},
  title = {Summarizing Source Code using a Neural Attention Model},
  year = {2016}
}

@misc{ref11,
  author = {Xing Hu and Li Ge and Xin Xia and David Lo and Shuai Lu and Zhi Jin},
  title = {Summarizing Source Code with Transferred API Knowledge},
  year = {2018}
}

@misc{ref12,
  author = {Xing Hu and Ge Li and Xin Xia and David Lo and Zhi Jin},
  title = {Deep code comment generation},
  year = {2018}
}

@misc{ref13,
  author = {Yao Wan and Zhou Zhao and Min Yang and Guandong Xu and Haochao Ying and Jian Wu and Philip S. Yu},
  title = {Improving automatic source code summarization via deep reinforcement learning},
  year = {2018}
}

@misc{ref14,
  author = {Jacob Devlin and Ming‐Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2018}
}

@misc{ref15,
  author = {Xing Hu and Ge Li and Xin Xia and David Lo and Zhi Jin},
  title = {Deep code comment generation with hybrid lexical and syntactical information},
  year = {2019}
}

@misc{ref16,
  author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  title = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  year = {2019}
}

@misc{ref17,
  author = {Alexander LeClair and Siyuan Jiang and Collin McMillan},
  title = {A Neural Model for Generating Natural Language Summaries of Program Subroutines},
  year = {2019}
}

@misc{ref18,
  author = {Miltiadis Allamanis},
  title = {The adverse effects of code duplication in machine learning models of code},
  year = {2019}
}

@misc{ref19,
  author = {Zhongxin Liu and Xin Xia and Christoph Treude and David Lo and Shanping Li},
  title = {Automatic Generation of Pull Request Descriptions},
  year = {2019}
}

@misc{ref20,
  author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  title = {Longformer: The Long-Document Transformer},
  year = {2020}
}

@misc{ref21,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {A Transformer-based Approach for Source Code Summarization},
  year = {2020}
}

@misc{ref22,
  author = {Alexander LeClair and Sakib Haque and Lingfei Wu and Collin McMillan},
  title = {Improved Code Summarization via a Graph Neural Network},
  year = {2020}
}

@misc{ref23,
  author = {Jian Zhang and Xu Wang and Hongyu Zhang and Hailong Sun and Xudong Liu},
  title = {Retrieval-based neural source code summarization},
  year = {2020}
}

@misc{ref24,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref25,
  author = {Xiaohan Yu and Quzhe Huang and Zheng Wang and Yansong Feng and Dongyan Zhao},
  title = {Towards Context-Aware Code Comment Generation},
  year = {2020}
}

@misc{ref26,
  author = {Jiezhong Qiu and Hao Ma and Omer Levy and Wen-tau Yih and Sinong Wang and Jie Tang},
  title = {Blockwise Self-Attention for Long Document Understanding},
  year = {2020}
}

@misc{ref27,
  author = {Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and A. Svyatkovskiy and Sheng‐Yu Fu and Michele Tufano and Shao Kun Deng and Colin B. Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
  title = {GraphCodeBERT: Pre-training Code Representations with Data Flow},
  year = {2021}
}

@misc{ref28,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {Unified Pre-training for Program Understanding and Generation},
  year = {2021}
}

@misc{ref29,
  author = {Chuhan Wu and Fangzhao Wu and Tao Qi and Yongfeng Huang},
  title = {Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling},
  year = {2021}
}

@misc{ref30,
  author = {Ensheng Shi and Yanlin Wang and Lun Du and Hongyu Zhang and Shi Han and Dongmei Zhang and Hongbin Sun},
  title = {CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees},
  year = {2021}
}

@misc{ref31,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
  title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  year = {2021}
}

@misc{ref32,
  author = {Zi Gong and Cuiyun Gao and Yasheng Wang and Wenchao Gu and Yun Peng and Zenglin Xu},
  title = {Source Code Summarization with Structural Relative Position Guided Transformer},
  year = {2022}
}

@misc{ref33,
  author = {Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},
  title = {UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  year = {2022}
}

@misc{ref34,
  author = {Y Wang and Yu Dong and Xuesong Lu and Aoying Zhou},
  title = {GypSum},
  year = {2022}
}

@misc{ref35,
  author = {Mandy Guo and Joshua Ainslie and David Uthus and Santiago Ontañón and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},
  title = {LongT5: Efficient Text-To-Text Transformer for Long Sequences},
  year = {2022}
}

@misc{ref36,
  author = {Tingting Liu and Chengyu Wang and Cen Chen and Ming Gao and Aoying Zhou},
  title = {Understanding Long Programming Languages with Structure-Aware Sparse Attention},
  year = {2022}
}

@misc{ref37,
  author = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontañón and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  title = {Big Bird: Transformers for Longer Sequences},
  year = {2020}
}

@misc{ref38,
  author = {Xuejian Gao and Xue Jiang and Qiong Wu and Xiao Wang and Chen Lyu and Lei Lyu},
  title = {GT-SimNet: Improving code automatic summarization via multi-modal similarity networks},
  year = {2022}
}

@misc{ref39,
  author = {Charles Condevaux and Sébastien Harispe},
  title = {LSG Attention: Extrapolation of Pretrained Transformers to Long Sequences},
  year = {2023}
}

@misc{ref40,
  author = {Yuexiu Gao and Chen Lyu},
  title = {M2TS},
  year = {2022}
}

@misc{ref41,
  author = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  title = {Generating Long Sequences with Sparse Transformers},
  year = {2019}
}

@misc{ref42,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

