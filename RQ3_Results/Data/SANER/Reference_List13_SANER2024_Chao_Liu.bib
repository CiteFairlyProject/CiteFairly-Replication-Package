@misc{ref1,
  author = {Jack Herrington},
  title = {Code Generation in Action},
  year = {2003}
}

@misc{ref2,
  author = {Kishore Papineni and Salim Roukos and Todd J. Ward and Wei-Jing Zhu},
  title = {BLEU},
  year = {2001}
}

@misc{ref3,
  author = {Richard Zens and Franz Josef Och and Hermann Ney},
  title = {Phrase-Based Statistical Machine Translation},
  year = {2002}
}

@misc{ref4,
  author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  title = {Sequence to Sequence Learning with Neural Networks},
  year = {2014}
}

@misc{ref5,
  author = {Jacob Devlin and Ming‐Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2018}
}

@misc{ref6,
  author = {Daya Guo and Duyu Tang and Nan Duan and Ming Zhou and Jian Yin},
  title = {Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing},
  year = {2019}
}

@misc{ref7,
  author = {Diego Marcílio and Rodrigo Bonifácio and Eduardo Monteiro and Edna Dias Canedo and Welder Luz and Gustavo Pinto},
  title = {Are Static Analysis Violations Really Fixed? A Closer Look at Realistic Usage of SonarQube},
  year = {2019}
}

@misc{ref8,
  author = {Srinivasan Iyer and Ioannis Konstas and Alvin Cheung and Luke Zettlemoyer},
  title = {Mapping Language to Code in Programmatic Context},
  year = {2018}
}

@misc{ref9,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = {2019}
}

@misc{ref10,
  author = {Valentina Lenarduzzi and Francesco Lomio and Heikki Huttunen and Davide Taibi},
  title = {Are SonarQube Rules Inducing Bugs?},
  year = {2020}
}

@misc{ref11,
  author = {Julián Salazar and Davis Liang and Toan Nguyen and Katrin Kirchhoff},
  title = {Masked Language Model Scoring},
  year = {2020}
}

@misc{ref12,
  author = {Frank F. Xu and Zhengbao Jiang and Pengcheng Yin and Bogdan Vasilescu and Graham Neubig},
  title = {Incorporating External Knowledge through Pre-training for Natural Language to Code Generation},
  year = {2020}
}

@misc{ref13,
  author = {Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
  title = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  year = {2020}
}

@misc{ref14,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref15,
  author = {Alexey Svyatkovskiy and Shao Kun Deng and Sheng‐Yu Fu and Neel Sundaresan},
  title = {IntelliCode compose: code generation using transformer},
  year = {2020}
}

@misc{ref16,
  author = {Jie Zhou and Junfeng Tian and Rui Wang and Yuanbin Wu and Wenming Xiao and Liang He},
  title = {SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis},
  year = {2020}
}

@misc{ref17,
  author = {Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and A. Svyatkovskiy and Sheng‐Yu Fu and Michele Tufano and Shao Kun Deng and Colin B. Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
  title = {GraphCodeBERT: Pre-training Code Representations with Data Flow},
  year = {2021}
}

@misc{ref18,
  author = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and A. Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Sheng‐Yu Fu and Shujie Liu},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  year = {2021}
}

@misc{ref19,
  author = {Antonio Mastropaolo and Simone Scalabrino and Nathan Cooper and David N. Palacio and Denys Poshyvanyk and Rocco Oliveto and Gabriele Bavota},
  title = {Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},
  year = {2021}
}

@misc{ref20,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {Unified Pre-training for Program Understanding and Generation},
  year = {2021}
}

@misc{ref21,
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pondé de Oliveira Pinto and Jared Kaplan and Harrison Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Łukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth A. Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and I. Babuschkin and Suchir Balaji and Shantanu Jain and William S. Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title = {Evaluating Large Language Models Trained on Code},
  year = {2021}
}

@misc{ref22,
  author = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year = {2022}
}

@misc{ref23,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
  title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  year = {2021}
}

@misc{ref24,
  author = {Vivian Liu and Lydia B. Chilton},
  title = {Design Guidelines for Prompt Engineering Text-to-Image Generative Models},
  year = {2022}
}

@misc{ref25,
  author = {Stephan Lukasczyk and Florian Kroiß and Gordon Fraser},
  title = {An empirical study of automated unit test generation for Python},
  year = {2023}
}

@misc{ref26,
  author = {Paras Jain and Ajay N. Jain and Tianjun Zhang and Pieter Abbeel and Joseph E. Gonzalez and Ion Stoica},
  title = {Contrastive Code Representation Learning},
  year = {2021}
}

@misc{ref27,
  author = {Jason Lee and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. and Quoc V. Le and Denny Zhou},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year = {2022}
}

@misc{ref28,
  author = {Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},
  title = {UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  year = {2022}
}

@misc{ref29,
  author = {Gabriel Poesia and Oleksandr Polozov and Vu Le and Ashish Tiwari and Gustavo Soares and Christopher Meek and Sumit Gulwani},
  title = {Synchromesh: Reliable code generation from pre-trained language models},
  year = {2022}
}

@misc{ref30,
  author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  title = {Training language models to follow instructions with human feedback},
  year = {2022}
}

@misc{ref31,
  author = {Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
  title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  year = {2022}
}

@misc{ref32,
  author = {Albert Ziegler and Eirini Kalliamvakou and X. Alice Li and Andrew P. Rice and Devon Rifkin and Shawn Simister and Ganesh Sittampalam and Edward Aftandilian},
  title = {Productivity assessment of neural code completion},
  year = {2022}
}

@misc{ref33,
  author = {Jason Lee and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  title = {Emergent Abilities of Large Language Models},
  year = {2022}
}

@misc{ref34,
  author = {Changan Niu and Chuanyi Li and Vincent Ng and Jidong Ge and LiGuo Huang and Bin Luo},
  title = {SPT-code},
  year = {2022}
}

@misc{ref35,
  author = {Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Sachdeva and Peter Clark and Chitta Baral and Ashwin Kalyan},
  title = {NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  year = {2022}
}

@misc{ref36,
  author = {Beliz Gunel and Jingfei Du and Alexis Conneau and Ves Stoyanov},
  title = {Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning},
  year = {2020}
}

@misc{ref37,
  author = {Chaozheng Wang and Yuanhang Yang and Cuiyun Gao and Yün Peng and Hongyu Zhang and Michael R. Lyu},
  title = {No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence},
  year = {2022}
}

@misc{ref38,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{ref39,
  author = {T. B. Brown and Benjamin F. Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey C.S. Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric J. Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack A. Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {Language Models are Few-Shot Learners},
  year = {2020}
}

@misc{ref40,
  author = {Shangqing Liu and Yanzhou Li and Yang Liu},
  title = {CommitBART: A Large Pre-trained Model for GitHub Commits},
  year = {2022}
}

@misc{ref41,
  author = {Dogu Araci},
  title = {FinBERT: Financial Sentiment Analysis with Pre-trained Language Models},
  year = {2019}
}

@misc{ref42,
  author = {Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
  title = {Automatic Chain of Thought Prompting in Large Language Models},
  year = {2022}
}

@misc{ref43,
  author = {Dheeraj Rajagopal and Vivek Khetan and Bogdan Sacaleanu and Anatole Gershman and Andrew Fano and Eduard Hovy},
  title = {Template Filling for Controllable Commonsense Reasoning},
  year = {2021}
}

@misc{ref44,
  author = {Anas Alokla and Walaa Gad and Waleed Nazih and Mustafa Aref and Abdel-Badeeh M. Salem},
  title = {Pseudocode Generation from Source Code Using the BART Model},
  year = {2022}
}

@misc{ref45,
  author = {Qing Huang and Zhiqiang Yuan and Zhenchang Xing and Xiwei Xu and Liming Zhu and Qinghua Lu},
  title = {Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code},
  year = {2022}
}

@misc{ref46,
  author = {Qing Lyu and Josh Tan and Michael E. Zapadka and Janardhana Ponnatapura and Chuang Niu and Kyle J. Myers and Ge Wang and Christopher T. Whitlow},
  title = {Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential},
  year = {2023}
}

@misc{ref47,
  author = {Michael Reiß},
  title = {Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark},
  year = {2023}
}

@misc{ref48,
  author = {Anai N. Kothari},
  title = {ChatGPT, Large Language Models, and Generative AI as Future Augments of Surgical Cancer Care},
  year = {2023}
}

@misc{ref49,
  author = {Partha Pratim Ray},
  title = {ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope},
  year = {2023}
}

@misc{ref50,
  author = {Michael Xieyang Liu and Advait Sarkar and Carina Negreanu and Benjamin G. Zorn and Jack M. Williams and Neil Toronto and Andrew D. Gordon},
  title = {“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models},
  year = {2023}
}

@misc{ref51,
  author = {Brent N. Reeves and Sami Sarsa and James Prather and Paul Denny and Brett A. Becker and Arto Hellas and Bailey Kimmel and Garrett Powell and Juho Leinonen},
  title = {Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations},
  year = {2023}
}

@misc{ref52,
  author = {Shangqing Liu and Bozhi Wu and Xiaofei Xie and Guozhu Meng and Yang Liu},
  title = {ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning},
  year = {2023}
}

@misc{ref53,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

@misc{ref54,
  author = {Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},
  title = {Reasoning with Language Model Prompting: A Survey},
  year = {2023}
}

@misc{ref55,
  author = {Mirac Süzgün and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. and Denny Zhou and Jason Lee},
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  year = {2023}
}

@misc{ref56,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

