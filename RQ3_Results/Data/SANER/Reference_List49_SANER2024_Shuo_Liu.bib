@misc{ref1,
  author = {Irina Ioana Brudaru and Andreas Zeller},
  title = {What is the long-term impact of changes?},
  year = {2008}
}

@misc{ref2,
  author = {Luis Fernando Cortes-Coy and Mario Linares‐Vásquez and Jairo Aponte and Denys Poshyvanyk},
  title = {On Automatically Generating Commit Messages via Summarization of Source Code Changes},
  year = {2014}
}

@misc{ref3,
  author = {Kishore Papineni and Salim Roukos and Todd J. Ward and Wei-Jing Zhu},
  title = {BLEU},
  year = {2001}
}

@misc{ref4,
  author = {Yasutaka Kamei and Emad Shihab and Bram Adams and Ahmed E. Hassan and Audris Mockus and Anand Sinha and Naoyasu Ubayashi},
  title = {A large-scale empirical study of just-in-time quality assurance},
  year = {2012}
}

@misc{ref5,
  author = {Chin-Yew Lin},
  title = {ROUGE: A Package for Automatic Evaluation of Summaries},
  year = {2004}
}

@misc{ref6,
  author = {Gabriel Murray and Steve Renals and Jean Carletta and Johanna D. Moore},
  title = {Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  year = {2005}
}

@misc{ref7,
  author = {Serhii Havrylov and Ivan Titov},
  title = {5th International Conference on Learning Representations (ICLR 17)},
  year = {2017}
}

@misc{ref8,
  author = {Pedro Martins and Rohan Achar and Cristina Videira Lopes},
  title = {50K-C},
  year = {2018}
}

@misc{ref9,
  author = {},
  title = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  year = {2018}
}

@misc{ref10,
  author = {Ian Tenney and Dipanjan Das and Ellie Pavlick},
  title = {BERT Rediscovers the Classical NLP Pipeline},
  year = {2019}
}

@misc{ref11,
  author = {Thong Hoang and Hoa Khanh Dam and Yasutaka Kamei and David Lo and Naoyasu Ubayashi},
  title = {DeepJIT: An End-to-End Deep Learning Framework for Just-in-Time Defect Prediction},
  year = {2019}
}

@misc{ref12,
  author = {Siyuan Jiang and Ameer Armaly and Collin McMillan},
  title = {Automatically generating commit messages from diffs using neural machine translation},
  year = {2017}
}

@misc{ref13,
  author = {Shengbin Xu and Yuan Yao and Xu Feng and Tianxiao Gu and Hanghang Tong and Jian Lü},
  title = {Commit Message Generation for Source Code Changes},
  year = {2019}
}

@misc{ref14,
  author = {Thong Hoang and Julia Lawall and Yuan Tian and Richard J. Oentaryo and David Lo},
  title = {PatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the Linux Kernel},
  year = {2019}
}

@misc{ref15,
  author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  year = {2020}
}

@misc{ref16,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref17,
  author = {Alexey Svyatkovskiy and Shao Kun Deng and Sheng‐Yu Fu and Neel Sundaresan},
  title = {IntelliCode compose: code generation using transformer},
  year = {2020}
}

@misc{ref18,
  author = {},
  title = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  year = {2020}
}

@misc{ref19,
  author = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and A. Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Sheng‐Yu Fu and Shujie Liu},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  year = {2021}
}

@misc{ref20,
  author = {Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei Chang},
  title = {Unified Pre-training for Program Understanding and Generation},
  year = {2021}
}

@misc{ref21,
  author = {Chanathip Pornprasit and Chakkrit Tantithamthavorn},
  title = {JITLine: A Simpler, Better, Faster, Finer-grained Just-In-Time Defect Prediction},
  year = {2021}
}

@misc{ref22,
  author = {Ruidan He and Linlin Liu and Hai Ye and Qingyu Tan and Bosheng Ding and Liying Cheng and Jia-Wei Low and Lidong Bing and Luo Si},
  title = {On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation},
  year = {2021}
}

@misc{ref23,
  author = {Zhengran Zeng and Yuqun Zhang and Haotian Zhang and Lingming Zhang},
  title = {Deep just-in-time defect prediction: how far are we?},
  year = {2021}
}

@misc{ref24,
  author = {Wei Tao and Yanlin Wang and Ensheng Shi and Lun Du and Shi Han and Hongyu Zhang and Dongmei Zhang and Wenqiang Zhang},
  title = {On the Evaluation of Commit Message Generation Models: An Experimental Study},
  year = {2021}
}

@misc{ref25,
  author = {Yuxian Gu and Xu Han and Zhiyuan Liu and Minlie Huang},
  title = {PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  year = {2022}
}

@misc{ref26,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
  title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  year = {2021}
}

@misc{ref27,
  author = {Steffen Herbold and Alexander Trautsch and Benjamin Ledel and Alireza Aghamohammadi and Taher Ahmed Ghaleb and Kuljit Kaur Chahal and Tim Bossenmaier and Bhaveet Nagaria and Philip Makedonski and Matin Nili Ahmadabadi and Kristóf Szabados and Helge Spieker and Matej Madeja and Nathaniel Hoy and Valentina Lenarduzzi and Shangwen Wang and Gema Rodríguez-Pérez and Ricardo Colomo‐Palacios and Roberto Verdecchia and Paramvir Singh and Yihao Qin and Debasish Chakroborti and Willard Davis and Vijay Walunj and Hongjun Wu and Diego Marcílio and Omar Alam and Abdullah Aldaeej and Idan Amit and Burak Turhan and Simon Eismann and Anna-Katharina Wickert and Ivano Malavolta and Matúš Sulír and Fatemeh H. Fard and Austin Z. Henley and Stratos Kourtzanidis and Eray Tüzün and Christoph Treude and Simin Maleki Shamasbi and Ivan Pashchenko and Marvin Wyrich and James A. Davis and Alexander Serebrenik and Ella Albrecht and Ethem Utku Aktaş and Daniel Strüber and Johannes Erbel},
  title = {A fine-grained data set and analysis of tangling in bug fixing commits},
  year = {2022}
}

@misc{ref28,
  author = {Anjan Karmakar and Romain Robbes},
  title = {What do pre-trained code models know about code?},
  year = {2021}
}

@misc{ref29,
  author = {Brian Lester and Rami Al‐Rfou and Noah Constant},
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  year = {2021}
}

@misc{ref30,
  author = {Zhensu Sun and Li Li and Liu Yan and Xiaoning Du and Li Li},
  title = {On the importance of building high-quality training datasets for neural code search},
  year = {2022}
}

@misc{ref31,
  author = {Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},
  title = {UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  year = {2022}
}

@misc{ref32,
  author = {Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
  title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  year = {2022}
}

@misc{ref33,
  author = {Robert Dyer and Hoan Anh Nguyen and Hridesh Rajan and Tien N. Nguyen},
  title = {Boa: A language and infrastructure for analyzing ultra-large-scale software repositories},
  year = {2013}
}

@misc{ref34,
  author = {Bo Lin and Shangwen Wang and Ming Wen and Xiaoguang Mao},
  title = {Context-Aware Code Change Embedding for Better Patch Correctness Assessment},
  year = {2022}
}

@misc{ref35,
  author = {Zhen Yang and Jacky Keung and Xiao Yu and Yan Xiao and Zhi Jin and Jingyu Zhang},
  title = {On the Significance of Category Prediction for Code-Comment Synchronization},
  year = {2022}
}

@misc{ref36,
  author = {Ming Zhu and Aneesh Jain and Karthik Suresh and Roshan Ravindran and Sindhu Tipirneni and Chandan K. Reddy},
  title = {XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence},
  year = {2022}
}

@misc{ref37,
  author = {Jinhao Dong and Yiling Lou and Qihao Zhu and Zeyu Sun and Zhilin Li and Wenjie Zhang and Dan Hao},
  title = {FIRA},
  year = {2022}
}

@misc{ref38,
  author = {Zhengran Zeng and Hanzhuo Tan and Haotian Zhang and Jing Li and Yuqun Zhang and Lingming Zhang},
  title = {An extensive study on pre-trained models for program understanding and generation},
  year = {2022}
}

@misc{ref39,
  author = {Chaozheng Wang and Yuanhang Yang and Cuiyun Gao and Yün Peng and Hongyu Zhang and Michael R. Lyu},
  title = {No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence},
  year = {2022}
}

@misc{ref40,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{ref41,
  author = {Zhiyu Li and Shuai Lu and Daya Guo and Nan Duan and Shailesh Jannu and Grant Jenks and Deep Majumder and Jared Green and A. Svyatkovskiy and Sheng‐Yu Fu and Neel Sundaresan},
  title = {Automating code review activities by large-scale pre-training},
  year = {2022}
}

@misc{ref42,
  author = {Chao Ni and Wei Wang and Kaiwen Yang and Xin Xia and Kui Liu and David Lo},
  title = {The best of both worlds: integrating semantic features with expert features for defect prediction and localization},
  year = {2022}
}

@misc{ref43,
  author = {Yichen Xu and Yanqiao Zhu},
  title = {A Survey on Pretrained Language Models for Neural Code Intelligence},
  year = {2022}
}

@misc{ref44,
  author = {Divyam Goel and Ramansh Grover and Fatemeh H. Fard},
  title = {On the cross-modal transfer from natural language to code through adapter modules},
  year = {2022}
}

@misc{ref45,
  author = {Jiaan Wang and Fandong Meng and Duo Zheng and Yunlong Liang and Zhixu Li and Jianfeng Qu and Jie Zhou},
  title = {A Survey on Cross-Lingual Summarization},
  year = {2022}
}

@misc{ref46,
  author = {Ning Ding and Yujia Qin and Guang Yang and Fuchao Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Hai-Tao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juanzi Li and Maosong Sun},
  title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
  year = {2023}
}

@misc{ref47,
  author = {Vladislav Lialin and Vijeta Deshpande and Anna Rumshisky},
  title = {Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning},
  year = {2023}
}

@misc{ref48,
  author = {Zihao Fu and Haoran Yang and Anthony Man–Cho So and Wai Lam and Lidong Bing and Nigel Collier},
  title = {On the Effectiveness of Parameter-Efficient Fine-Tuning},
  year = {2023}
}

@misc{ref49,
  author = {Zhongxin Liu and Zhijie Tang and Xin Xia and Xiaohu Yang},
  title = {CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back},
  year = {2023}
}

@misc{ref50,
  author = {Caroline Lemieux and Jeevana Priya Inala and Shuvendu K. Lahiri and Siddhartha Sen},
  title = {CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models},
  year = {2023}
}

@misc{ref51,
  author = {Deze Wang and Boxing Chen and Shanshan Li and Wei Luo and Shaoliang Peng and Wei Dong and Xiangke Liao},
  title = {One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization},
  year = {2023}
}

@misc{ref52,
  author = {Chunqiu Steven Xia and Yuxiang Wei and Lingming Zhang},
  title = {Automated Program Repair in the Era of Large Pre-trained Language Models},
  year = {2023}
}

@misc{ref53,
  author = {Changan Niu and Chuanyi Li and Vincent Ng and Dongxiao Chen and Jidong Ge and Bin Luo},
  title = {An Empirical Comparison of Pre-Trained Models of Source Code},
  year = {2023}
}

@misc{ref54,
  author = {Sungmin Kang and Juyeon Yoon and Shin Yoo},
  title = {Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction},
  year = {2023}
}

@misc{ref55,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

@misc{ref56,
  author = {Ensheng Shi and Yanlin Wang and Weiliang Tao and Lun Du and Hongyu Zhang and Han Shi and Dongmei Zhang and Hongbin Sun},
  title = {RACE: Retrieval-augmented Commit Message Generation},
  year = {2022}
}

@misc{ref57,
  author = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  title = {Code Llama: Open Foundation Models for Code},
  year = {2023}
}

@misc{ref58,
  author = {Zhen Yang and Jacky Keung and Zeyu Sun and Yunfei Zhao and Ge Li and Zhi Jin and Shuo Liu and Yishu Li},
  title = {Improving domain-specific neural code generation with few-shot meta-learning},
  year = {2023}
}

@misc{ref59,
  author = {Bo Lin and Shangwen Wang and Zhongxin Liu and Yepang Liu and Xin Xia and Xiaoguang Mao},
  title = {CCT5: A Code-Change-Oriented Pre-trained Model},
  year = {2023}
}

@misc{ref60,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

@misc{ref61,
  author = {Iman Saberi and Fatemeh H. Fard and Fuxiang Chen},
  title = {Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering},
  year = {2024}
}

