@misc{ref1,
  author = {Ciprian Chelba and Alex Acero},
  title = {Adaptation of maximum entropy capitalizer: Little data can help a lot},
  year = {2005}
}

@misc{ref2,
  author = {Alberto Bacchelli and Michele Lanza and Romain Robbes},
  title = {Linking e-mails and source code artifacts},
  year = {2010}
}

@misc{ref3,
  author = {Laurens van der Maaten and Geoffrey E. Hinton},
  title = {Visualizing Data using t-SNE},
  year = {2008}
}

@misc{ref4,
  author = {Walid Maalej and Zijad Kurtanović and Hadeer Nabil and Christoph Stanik},
  title = {On the automatic classification of app reviews},
  year = {2016}
}

@misc{ref5,
  author = {Simone Scalabrino and Mario Linares‐Vásquez and Denys Poshyvanyk and Rocco Oliveto},
  title = {Improving code readability models with textual features},
  year = {2016}
}

@misc{ref6,
  author = {Qing He and Beijun Shen and Yuting Chen},
  title = {Software Defect Prediction Using Semi-Supervised Learning with Change Burst Information},
  year = {2016}
}

@misc{ref7,
  author = {Serhii Havrylov and Ivan Titov},
  title = {5th International Conference on Learning Representations (ICLR 17)},
  year = {2017}
}

@misc{ref8,
  author = {Simone Scalabrino and Gabriele Bavota and Barbara Russo and Massimiliano Di Penta and Rocco Oliveto},
  title = {Listening to the Crowd for the Release Planning of Mobile Apps},
  year = {2017}
}

@misc{ref9,
  author = {Sarah Fakhoury and Venera Arnaoudova and Cedric Noiseux and Foutse Khomh and Giuliano Antoniol},
  title = {Keep it simple: Is deep learning good for linguistic smell detection?},
  year = {2018}
}

@misc{ref10,
  author = {Simone Scalabrino and Mario Linares‐Vásquez and Rocco Oliveto and Denys Poshyvanyk},
  title = {A comprehensive model for code readability},
  year = {2018}
}

@misc{ref11,
  author = {Stephen José Hanson and Jack D. Cowan and C. Lee Giles},
  title = {Advances in Neural Information Processing Systems 5, [NIPS Conference]},
  year = {1992}
}

@misc{ref12,
  author = {Naftali Tishby and Noga Zaslavsky},
  title = {Deep learning and the information bottleneck principle},
  year = {2015}
}

@misc{ref13,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = {2019}
}

@misc{ref14,
  author = {Xiaoxue Ren and Zhenchang Xing and Xin Xia and David Lo and Xinyu Wang and John Grundy},
  title = {Neural Network-based Detection of Self-Admitted Technical Debt},
  year = {2019}
}

@misc{ref15,
  author = {Xiang Lisa Li and Jason Eisner},
  title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
  year = {2019}
}

@misc{ref16,
  author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clément Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Jamie Brew},
  title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  year = {2019}
}

@misc{ref17,
  author = {Peter West and Ari Holtzman and Jan Buys and Yejin Choi},
  title = {BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle},
  year = {2019}
}

@misc{ref18,
  author = {Jagriti Sikka and Kushal Satya and Yaman Kumar and Shagun Uppal and Rajiv Ratn Shah and Roger Zimmermann},
  title = {Learning Based Methods for Code Runtime Complexity Prediction},
  year = {2020}
}

@misc{ref19,
  author = {Marius Mosbach and Maksym Andriushchenko and Dietrich Klakow},
  title = {On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
  year = {2020}
}

@misc{ref20,
  author = {Ivan Provilkov and Dmitrii Emelianenko and Elena Voita},
  title = {BPE-Dropout: Simple and Effective Subword Regularization},
  year = {2020}
}

@misc{ref21,
  author = {Qing Mi and Yan Xiao and Zhi Cai and Xibin Jia},
  title = {The effectiveness of data augmentation in code readability classification},
  year = {2020}
}

@misc{ref22,
  author = {Ting Zhang and Bowen Xu and Ferdian Thung and Stefanus Agus Haryono and David Lo and Lingxiao Jiang},
  title = {Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?},
  year = {2020}
}

@misc{ref23,
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  year = {2020}
}

@misc{ref24,
  author = {Daniel Grießhaber and Johannes Maucher and Ngoc Thang Vu},
  title = {Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning},
  year = {2020}
}

@misc{ref25,
  author = {Fabio Calefato and Filippo Lanubile and Federico Maiorano and Nicole Novielli},
  title = {Sentiment Polarity Detection for Software Development},
  year = {2017}
}

@misc{ref26,
  author = {Timo Schick and Hinrich Schütze},
  title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  year = {2021}
}

@misc{ref27,
  author = {Md Rafiqul Islam Rabin and Aftab Hussain and Mohammad Amin Alipour and Vincent J. Hellendoorn},
  title = {Memorization and Generalization in Neural Code Intelligence Models},
  year = {2022}
}

@misc{ref28,
  author = {Julian Aron Prenner and Romain Robbes},
  title = {Making the most of small Software Engineering datasets with modern machine learning},
  year = {2021}
}

@misc{ref29,
  author = {Deze Wang and Zhouyang Jia and Shanshan Li and Yue Yu and Yun Xiong and Wei Dong and Xiangke Liao},
  title = {Bridging pre-trained models and downstream tasks for source code understanding},
  year = {2022}
}

@misc{ref30,
  author = {Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},
  title = {UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  year = {2022}
}

@misc{ref31,
  author = {Changan Niu and Chuanyi Li and Vincent Ng and Jidong Ge and LiGuo Huang and Bin Luo},
  title = {SPT-code},
  year = {2022}
}

@misc{ref32,
  author = {Rosalia Tufano and Simone Masiero and Antonio Mastropaolo and Luca Pascarella and Denys Poshyvanyk and Gabriele Bavota},
  title = {Using pre-trained models to boost code review automation},
  year = {2022}
}

@misc{ref33,
  author = {Jesse Dodge and Gabriel Ilharco and Roy Schwartz and Ali Farhadi and Hannaneh Hajishirzi and Noah A. Smith},
  title = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping},
  year = {2020}
}

@misc{ref34,
  author = {Lynton Ardizzone and Radek Mackowiak and Carsten Rother and Ullrich Köthe},
  title = {Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification},
  year = {2020}
}

@misc{ref35,
  author = {L. N. Kanal and J. F. Lemmer and Andrew P. Sage},
  title = {Uncertainty in Artificial Intelligence},
  year = {1987}
}

@misc{ref36,
  author = {Ravid Shwartz-Ziv and Naftali Tishby},
  title = {Opening the Black Box of Deep Neural Networks via Information},
  year = {2017}
}

@misc{ref37,
  author = {Jason Phang and Thibault Févry and Samuel R. Bowman},
  title = {Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
  year = {2018}
}

@misc{ref38,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  year = {2017}
}

@misc{ref39,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

