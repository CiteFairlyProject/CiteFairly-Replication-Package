@misc{ref1,
  author = {Rodrigo Morales and Shane McIntosh and Foutse Khomh},
  title = {Do code review practices impact design quality? A case study of the Qt, VTK, and ITK projects},
  year = {2015}
}

@misc{ref2,
  author = {Oleksii Kononenko and Olga Baysal and Michael W. Godfrey},
  title = {Code review quality},
  year = {2016}
}

@misc{ref3,
  author = {Tobias Baum and Olga Liskin and Kai Niklas and Kurt Schneider},
  title = {Factors influencing code review processes in industry},
  year = {2016}
}

@misc{ref4,
  author = {Caitlin Sadowski and Emma Söderberg and Luke Church and Michal Sipko and Alberto Bacchelli},
  title = {Modern code review},
  year = {2018}
}

@misc{ref5,
  author = {Michele Tufano and Jevgenija Pantiuchina and Cody Watson and Gabriele Bavota and Denys Poshyvanyk},
  title = {On Learning Meaningful Code Changes Via Neural Machine Translation},
  year = {2019}
}

@misc{ref6,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year = {2019}
}

@misc{ref7,
  author = {Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
  title = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  year = {2020}
}

@misc{ref8,
  author = {Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and A. Svyatkovskiy and Sheng‐Yu Fu and Michele Tufano and Shao Kun Deng and Colin B. Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
  title = {GraphCodeBERT: Pre-training Code Representations with Data Flow},
  year = {2021}
}

@misc{ref9,
  author = {Rosalia Tufano and Luca Pascarella and Michele Tufano and Denys Poshyvanyk and Gabriele Bavota},
  title = {Towards Automating Code Review Activities},
  year = {2021}
}

@misc{ref10,
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
  title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  year = {2021}
}

@misc{ref11,
  author = {Patanamon Thongtanunam and Chanathip Pornprasit and Chakkrit Tantithamthavorn},
  title = {AutoTransform},
  year = {2022}
}

@misc{ref12,
  author = {Jacek Czerwonka and Michaela Greiler and J.M. Tilford},
  title = {Code Reviews Do Not Find Bugs. How the Current Code Review Best Practice Slows Us Down},
  year = {2015}
}

@misc{ref13,
  author = {Rosalia Tufano and Simone Masiero and Antonio Mastropaolo and Luca Pascarella and Denys Poshyvanyk and Gabriele Bavota},
  title = {Using pre-trained models to boost code review automation},
  year = {2022}
}

@misc{ref14,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2019}
}

@misc{ref15,
  author = {Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
  title = {CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  year = {2019}
}

