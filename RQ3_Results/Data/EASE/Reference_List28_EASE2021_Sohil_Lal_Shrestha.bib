@misc{ref1,
  author = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},
  title = {ImageNet: A large-scale hierarchical image database},
  year = {2009}
}

@misc{ref2,
  author = {Veselin Raychev and Martin Vechev and Eran Yahav},
  title = {Code completion with statistical language models},
  year = {2014}
}

@misc{ref3,
  author = {Sinno Jialin Pan and Qiang Yang},
  title = {A Survey on Transfer Learning},
  year = {2009}
}

@misc{ref4,
  author = {Xi Zheng and Christine Julien and Miryung Kim and Sarfraz Khurshid},
  title = {Perceptions on the State of the Art in Verification and Validation in Cyber-Physical Systems},
  year = {2015}
}

@misc{ref5,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention is All you Need},
  year = {2017}
}

@misc{ref6,
  author = {Sarah Fakhoury and Venera Arnaoudova and Cedric Noiseux and Foutse Khomh and Giuliano Antoniol},
  title = {Keep it simple: Is deep learning good for linguistic smell detection?},
  year = {2018}
}

@misc{ref7,
  author = {Chris Cummins and Pavlos Petoumenos and Alastair Murray and Hugh Leather},
  title = {Compiler fuzzing through deep learning},
  year = {2018}
}

@misc{ref8,
  author = {Shafiul Azam Chowdhury and Soumik Mohian and Sidharth Mehra and Siddhant Gawsane and Taylor T. Johnson and Christoph Csallner},
  title = {Automatically finding bugs in a commercial cyber-physical system development tool chain with SLforge},
  year = {2018}
}

@misc{ref9,
  author = {Qingying Chen and Minghui Zhou},
  title = {A neural framework for retrieval and summarization of source code},
  year = {2018}
}

@misc{ref10,
  author = {Xiao Liu and Xiaoting Li and Rupesh Prajapati and Dinghao Wu},
  title = {DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing},
  year = {2019}
}

@misc{ref11,
  author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
  title = {The Curious Case of Neural Text Degeneration},
  year = {2019}
}

@misc{ref12,
  author = {Romain Robbes and Andrea Janes},
  title = {Leveraging Small Software Engineering Data Sets with Pre-Trained Neural Networks},
  year = {2019}
}

@misc{ref13,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
  title = {Attention is All you Need},
  year = {2017}
}

@misc{ref14,
  author = {Ari Holtzman and Jan Buys and Leo Du and Maxwell Forbes and Yejin Choi},
  title = {The Curious Case of Neural Text Degeneration},
  year = {2020}
}

@misc{ref15,
  author = {Veselin Raychev and Martin Vechev and Eran Yahav},
  title = {Code completion with statistical language models},
  year = {2014}
}

@misc{ref16,
  author = {Muhammad Hammad and Önder Babur and Hamid Abdul Basit and Mark van den Brand},
  title = {DeepClone: Modeling Clones to Generate Code Predictions},
  year = {2020}
}

@misc{ref17,
  author = {Haoran Xu and Yongjun Wang and Shuhui Fan and Peidai Xie and Aizhi Liu},
  title = {DSmith: Compiler Fuzzing through Generative Deep Learning Model with Attention},
  year = {2020}
}

@misc{ref18,
  author = {Shafiul Azam Chowdhury and Sohil Lal Shrestha and Taylor T. Johnson and Christoph Csallner},
  title = {SLEMI},
  year = {2020}
}

@misc{ref19,
  author = {Yasir Hussain and Zhiqiu Huang and Yu Zhou and Senzhang Wang},
  title = {Deep Transfer Learning for Source Code Modeling},
  year = {2020}
}

@misc{ref20,
  author = {Muhammad Hammad and Önder Babur and Hamid Abdul Basit and Mark van den Brand},
  title = {DeepClone: Modeling Clones to Generate Code Predictions},
  year = {2020}
}

@misc{ref21,
  author = {Juan Cruz-Benito and Sanjay Vishwakarma and Francisco Martín-Fernández and Ismael Faro},
  title = {Automated Source Code Generation and Auto-Completion Using Deep Learning: Comparing and Discussing Current Language Model-Related Approaches},
  year = {2021}
}

